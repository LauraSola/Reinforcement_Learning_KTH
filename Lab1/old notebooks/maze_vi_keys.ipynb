{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright [2024] [KTH Royal Institute of Technology] \n",
    "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
    "# This file is part of the Computer Lab 1 for EL2805 - Reinforcement Learning.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import random\n",
    "\n",
    "# Implemented methods\n",
    "methods = ['DynProg', 'ValIter']\n",
    "\n",
    "# Some colours\n",
    "LIGHT_RED    = '#FFC4CC'\n",
    "LIGHT_GREEN  = '#95FD99'\n",
    "BLACK        = '#000000'\n",
    "WHITE        = '#FFFFFF'\n",
    "LIGHT_PURPLE = '#E8D0FF'\n",
    "LIGHT_ORANGE = '#FFDBBB'\n",
    "\n",
    "class Maze:\n",
    "\n",
    "    # Actions\n",
    "    STAY       = 0\n",
    "    MOVE_LEFT  = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP    = 3\n",
    "    MOVE_DOWN  = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "\n",
    "    # Reward values \n",
    "    STEP_REWARD = 0          #TODO\n",
    "    GOAL_REWARD = 10          #TODO\n",
    "    KEY_REWARD = 1           #TODO\n",
    "    IMPOSSIBLE_REWARD = -100    #TODO\n",
    "    MINOTAUR_REWARD = -100      #TODO\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze                     = maze\n",
    "        self.actions                  = self.__actions()\n",
    "        self.states, self.map         = self.__states()\n",
    "        self.n_actions                = len(self.actions)\n",
    "        self.n_states                 = len(self.states)\n",
    "        self.transition_probabilities = self.__transitions()\n",
    "        self.rewards                  = self.__rewards()\n",
    "\n",
    "    def __actions(self):\n",
    "        actions = dict()\n",
    "        actions[self.STAY]       = (0, 0)\n",
    "        actions[self.MOVE_LEFT]  = (0,-1)\n",
    "        actions[self.MOVE_RIGHT] = (0, 1)\n",
    "        actions[self.MOVE_UP]    = (-1,0)\n",
    "        actions[self.MOVE_DOWN]  = (1,0)\n",
    "        return actions\n",
    "\n",
    "    def __states(self):\n",
    "        \n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        s = 0\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                for k in range(self.maze.shape[0]):\n",
    "                    for l in range(self.maze.shape[1]):\n",
    "                        if self.maze[i,j] != 1:\n",
    "                            states[s] = ((i,j), (k,l), False)\n",
    "                            map[((i,j), (k,l), False)] = s\n",
    "                            s += 1\n",
    "                            states[s] = ((i,j), (k,l), True)\n",
    "                            map[((i,j), (k,l), True)] = s\n",
    "                            s += 1\n",
    "        \n",
    "        states[s] = 'Eaten'\n",
    "        map['Eaten'] = s\n",
    "        s += 1\n",
    "        \n",
    "        states[s] = 'Win'\n",
    "        map['Win'] = s\n",
    "        \n",
    "        return states, map\n",
    "\n",
    "\n",
    "    def __move(self, state, action):               \n",
    "        \"\"\" Makes a step in the maze, given a current position and an action. \n",
    "            If the action STAY or an inadmissible action is used, the player stays in place.\n",
    "        \n",
    "            :return list of tuples next_state: Possible states ((x,y), (x',y')) on the maze that the system can transition to.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.states[state] == 'Eaten' or self.states[state] == 'Win': # In these states, the game is over\n",
    "            return [self.states[state]]\n",
    "        \n",
    "        else: # Compute the future possible positions given current (state, action)\n",
    "            row_player = self.states[state][0][0] + self.actions[action][0] # Row of the player's next position \n",
    "            col_player = self.states[state][0][1] + self.actions[action][1] # Column of the player's next position \n",
    "            \n",
    "            # Is the player getting out of the limits of the maze or hitting a wall?\n",
    "            impossible_action_player = (row_player == -1 or row_player == self.maze.shape[0] or \\\n",
    "                                         col_player == -1 or col_player == self.maze.shape[1] or \\\n",
    "                                         self.maze[row_player, col_player] == 1 ) #TODO\n",
    "            \n",
    "            actions_minotaur = [[0, -1], [0, 1], [-1, 0], [1, 0]] # Possible moves for the Minotaur\n",
    "            rows_minotaur, cols_minotaur = [], []\n",
    "            for i in range(len(actions_minotaur)):\n",
    "                # Is the minotaur getting out of the limits of the maze?\n",
    "                impossible_action_minotaur = (self.states[state][1][0] + actions_minotaur[i][0] == -1) or \\\n",
    "                                             (self.states[state][1][0] + actions_minotaur[i][0] == self.maze.shape[0]) or \\\n",
    "                                             (self.states[state][1][1] + actions_minotaur[i][1] == -1) or \\\n",
    "                                             (self.states[state][1][1] + actions_minotaur[i][1] == self.maze.shape[1])\n",
    "            \n",
    "                if not impossible_action_minotaur:\n",
    "                    rows_minotaur.append(self.states[state][1][0] + actions_minotaur[i][0])\n",
    "                    cols_minotaur.append(self.states[state][1][1] + actions_minotaur[i][1])  \n",
    "          \n",
    "\n",
    "            # Based on the impossiblity check return the next possible states.\n",
    "            if impossible_action_player: # The action is not possible, so the player remains in place\n",
    "                states = []\n",
    "                for i in range(len(rows_minotaur)):\n",
    "                    \n",
    "                    if (self.states[state][0][0] == rows_minotaur[i] and self.states[state][0][1] == cols_minotaur[i]): # TODO: We met the minotaur\n",
    "                        states.append('Eaten')\n",
    "                    \n",
    "                    elif (self.maze[self.states[state][0][0], self.states[state][0][1]] == 2 and self.states[state][-1]): # TODO: We are at the exit state, without meeting the minotaur\n",
    "                        states.append('Win')\n",
    "                \n",
    "                    else: # The player remains in place, the minotaur moves randomly\n",
    "                        states.append(((self.states[state][0][0], self.states[state][0][1]), (rows_minotaur[i], cols_minotaur[i]), self.states[state][-1]))\n",
    "                \n",
    "                return states\n",
    "          \n",
    "            else: # The action is possible, the player and the minotaur both move\n",
    "                states = []\n",
    "                for i in range(len(rows_minotaur)):\n",
    "                \n",
    "                    if (row_player == rows_minotaur[i] and col_player == cols_minotaur[i]): # TODO: We met the minotaur\n",
    "                        states.append('Eaten')\n",
    "                    \n",
    "                    elif (self.maze[row_player, col_player] == 2 and self.states[state][-1]): # TODO:We are at the exit state, without meeting the minotaur\n",
    "                        states.append('Win')\n",
    "                    \n",
    "                    elif (self.maze[row_player, col_player] == 3): # The player gets the key\n",
    "                        states.append(((row_player, col_player), (rows_minotaur[i], cols_minotaur[i]), True))\n",
    "                    \n",
    "                    else: # The player moves, the minotaur moves randomly\n",
    "                        states.append(((row_player, col_player), (rows_minotaur[i], cols_minotaur[i]), self.states[state][-1]))                        \n",
    "                \n",
    "                return states\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __transitions(self):\n",
    "        \"\"\" Computes the transition probabilities for every state action pair.\n",
    "            :return numpy.tensor transition probabilities: tensor of transition\n",
    "            probabilities of dimension S*S*A\n",
    "        \"\"\"\n",
    "        # Initialize the transition probailities tensor (S,S,A)\n",
    "        dimensions = (self.n_states,self.n_states,self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "\n",
    "        # TODO: Compute the transition probabilities.\n",
    "  \n",
    "        for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    next_possible_states = self.__move(s,a)\n",
    "                    if 'Eaten' in next_possible_states or 'Win' in next_possible_states:\n",
    "                        min = next_possible_states.index('Eaten') if 'Eaten' in next_possible_states else next_possible_states.index('Win') \n",
    "                    else:\n",
    "                        dist = [np.linalg.norm(np.array(next_state[1])-np.array(next_state[0])) for next_state in next_possible_states]\n",
    "                        min = np.argmin(dist)\n",
    "\n",
    "                    # 35% chance of the minotaur moving in the same direction as the player, 65% chance of moving in a random direction\n",
    "                    for t in next_possible_states:\n",
    "                        uniform_prob = 1/len(next_possible_states)\n",
    "                        if t == next_possible_states[min]:\n",
    "                            transition_probabilities[self.map[t],s,a] = 0.35 + 0.65*uniform_prob\n",
    "                        else:\n",
    "                            transition_probabilities[self.map[t],s,a] = 0.65*uniform_prob \n",
    "\n",
    "                    # Normalize probabilities to ensure they sum to 1\n",
    "                    transition_probabilities[:, s, a] /= np.sum(transition_probabilities[:, s, a])\n",
    "   \n",
    "        return transition_probabilities\n",
    "\n",
    "\n",
    "\n",
    "    def __rewards(self):\n",
    "        \n",
    "        \"\"\" Computes the rewards for every state action pair \"\"\"\n",
    "\n",
    "        rewards = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                    \n",
    "                if self.states[s] == 'Eaten': # The player has been eaten\n",
    "                    rewards[s, a] = self.MINOTAUR_REWARD\n",
    "                \n",
    "                elif self.states[s] == 'Win': # The player has won\n",
    "                    rewards[s, a] = self.GOAL_REWARD\n",
    "\n",
    "                # if the player gets to the position of the key, he gets a reward if the minotaur is not there\n",
    "                elif self.states[s][-1] == False and self.maze[self.states[s][0][0], self.states[s][0][1]] == 3 and self.states[s][0] != self.states[s][1]:\n",
    "                    rewards[s, a] = self.KEY_REWARD\n",
    "                \n",
    "                else:                \n",
    "                    next_states = self.__move(s,a)\n",
    "                    next_s = next_states[0] # The reward does not depend on the next position of the minotaur, we just consider the first one\n",
    "                    \n",
    "                    if self.states[s][0] == next_s[0] and a != self.STAY: # The player hits a wall\n",
    "                        rewards[s, a] = self.IMPOSSIBLE_REWARD\n",
    "                    \n",
    "                    else: # Regular move\n",
    "                        rewards[s, a] = self.STEP_REWARD\n",
    "\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n",
    "    def simulate(self, start, policy, method):\n",
    "        \n",
    "        if method not in methods:\n",
    "            error = 'ERROR: the argument method must be in {}'.format(methods)\n",
    "            raise NameError(error)\n",
    "\n",
    "        path = list()\n",
    "        \n",
    "        if method == 'DynProg':\n",
    "            horizon = policy.shape[1] # Deduce the horizon from the policy shape\n",
    "            t = 0 # Initialize current time\n",
    "            s = self.map[start] # Initialize current state \n",
    "            path.append(start) # Add the starting position in the maze to the path\n",
    "            \n",
    "            while t < horizon - 1:\n",
    "                a = policy[s, t] # Move to next state given the policy and the current state\n",
    "                next_states = self.__move(s, a)\n",
    "                map_next_states = [self.map[state] for state in next_states]\n",
    "                # next_s = np.random.choice(map_next_states, p=self.transition_probabilities[map_next_states, s, a])[0]  # Choose the next state given the transition probabilities\n",
    "                next_s = np.random.choice(map_next_states) # Choose the next state randomly\n",
    "\n",
    "                path.append(self.states[next_s]) # Add the next state to the path\n",
    "                t +=1 # Update time and state for next iteration\n",
    "                s = next_s\n",
    "                \n",
    "        if method == 'ValIter': \n",
    "            t = 1 # Initialize current state, next state and time\n",
    "            s = self.map[start]\n",
    "            path.append(start) # Add the starting position in the maze to the path\n",
    "            next_states = self.__move(s, policy[s]) # Move to next state given the policy and the current state\n",
    "            map_next_states = [self.map[state] for state in next_states] # Map the next states\n",
    "            next_s = np.random.choice(map_next_states, p=self.transition_probabilities[map_next_states, s, policy[s]])  # Choose the next state given the transition probabilities\n",
    "            path.append(self.states[next_s]) # Add the next state to the path\n",
    "            \n",
    "            # horizon geometric mean 30\n",
    "            horizon = np.random.geometric(1/30) # Sample the horizon from a geometric distribution with mean 30\n",
    "            \n",
    "            # Loop while state is not the goal state\n",
    "            while s != next_s and t <= horizon:\n",
    "                s = next_s # Update state\n",
    "                next_states = self.__move(s, policy[s]) # Move to next state given the policy and the current state\n",
    "                map_next_states = [self.map[state] for state in next_states] # Map the next states\n",
    "                next_s = np.random.choice(map_next_states, p=self.transition_probabilities[map_next_states, s, policy[s]])  # Choose the next state given the transition probabilities\n",
    "                path.append(self.states[next_s]) # Add the next state to the path\n",
    "                t += 1 # Update time for next iteration\n",
    "        \n",
    "        return [path, horizon] # Return the horizon as well, to plot the histograms for the VI\n",
    "\n",
    "\n",
    "\n",
    "    def show(self):\n",
    "        print('The states are :')\n",
    "        print(self.states)\n",
    "        print('The actions are:')\n",
    "        print(self.actions)\n",
    "        print('The mapping of the states:')\n",
    "        print(self.map)\n",
    "        print('The rewards:')\n",
    "        print(self.rewards)\n",
    "\n",
    "\n",
    "\n",
    "    def possible_actions(self, state):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine the possible actions the player can take from a given state.\n",
    "\n",
    "        Args:\n",
    "            state (int): The current state index representing the player's position in the maze.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of valid actions (each as a tuple of row and column deltas) \n",
    "                that the player can take without exceeding maze boundaries or hitting walls.\n",
    "        \"\"\"\n",
    "\n",
    "        possible_actions = [0] #we can always stay in place\n",
    "        # possible_actions = []\n",
    "\n",
    "        if self.states[state]!= 'Eaten' and self.states[state]!= 'Win':\n",
    "\n",
    "            for action in range(1,self.n_actions):\n",
    "\n",
    "                row_player = self.states[state][0][0] + self.actions[action][0] # Row of the player's next position \n",
    "                col_player = self.states[state][0][1] + self.actions[action][1] # Column of the player's next position \n",
    "                \n",
    "                # Is the player getting out of the limits of the maze or hitting a wall?\n",
    "                impossible_action_player = (row_player == -1 or row_player == self.maze.shape[0] or \\\n",
    "                                            col_player == -1 or col_player == self.maze.shape[1] or \\\n",
    "                                            self.maze[row_player, col_player] == 1 )\n",
    "                \n",
    "\n",
    "                if not impossible_action_player:\n",
    "                    possible_actions.append(action)\n",
    "\n",
    "        return possible_actions\n",
    "\n",
    "\n",
    "\n",
    "    def observations(self, s, a):\n",
    "        \"\"\" Q learning algorithm:\n",
    "            Given the current state, and action, returns the next state and reward. If the state is terminal, returns done = True.\n",
    "                :return tuple next_state: next state\n",
    "                :return float reward: reward\n",
    "                :return bool done: True if the next state is terminal, False otherwise\n",
    "        \"\"\"\n",
    "        next_states = self.__move(s, a) #possible next states\n",
    "\n",
    "        map_next_states = [self.map[state] for state in next_states] # Map the next states\n",
    "        next_s = np.random.choice(map_next_states, p=self.transition_probabilities[map_next_states, s, a])  # Choose the next state given the transition probabilities\n",
    "\n",
    "        done = self.states[s] == 'Eaten' or self.states[s] == 'Win' # Check if the next state is terminal\n",
    "        reward = self.rewards[s, a] # Get the reward\n",
    "\n",
    "        # if reward > 0 print\n",
    "        if reward > 0:\n",
    "            print('Reward:', reward, 'State', self.states[s], 'Action', self.actions_names[a], 'Next state', self.states[next_s])\n",
    "\n",
    "        if a != 0:\n",
    "            print('Reward:', reward, 'State', self.states[s], 'Action', self.actions_names[a], 'Next state', self.states[next_s])\n",
    "\n",
    "        # if reward < 0:\n",
    "        #     print('Negative reward:', reward, 'State', self.states[s], 'Action', self.actions_names[a], 'Next state', self.states[next_s])\n",
    "        \n",
    "        #if the state is (0,7) print\n",
    "        # print(self.states)\n",
    "        if self.states[s][0] == (0,7):\n",
    "            print('State', self.states[s], 'Action', self.actions_names[a], 'Next state', self.states[next_s])\n",
    "        return next_s, reward, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RL_agent:\n",
    "\n",
    "    def __init__(self, env, epsilon, discount_factor, alpha ):\n",
    "        self.state = None\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.done = False\n",
    "\n",
    "        self.counter = np.zeros((self.env.n_states, self.env.n_actions))\n",
    "        self.Q = np.zeros((self.env.n_states, self.env.n_actions)) # can also be initialized by random small values or a constant high value\n",
    "    \n",
    "\n",
    "    def behavior_policy(self, s): ##epsilon-greedy policy\n",
    "\n",
    "        possible_actions = self.env.possible_actions(s)\n",
    "\n",
    "        if np.random.rand() < self.epsilon: ## explore\n",
    "            a = np.random.choice(possible_actions)\n",
    "\n",
    "        else: ## greedy action\n",
    "            # a = np.argmax(self.Q[s, possible_actions])\n",
    "            # print the mapped state\n",
    "            # print(self.env.states[s])\n",
    "            max_value = np.max(self.Q[s, possible_actions])\n",
    "            max_actions = [action for action in possible_actions if self.Q[s, action] == max_value]\n",
    "            a = np.random.choice(max_actions)\n",
    "\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_policy(self):\n",
    "\n",
    "        return np.argmax(self.Q, axis = 1)\n",
    "\n",
    "\n",
    "    def step_Q_learning(self):\n",
    "        \n",
    "        ## observations\n",
    "        s = self.state\n",
    "        # if state is terminal, skip\n",
    "        # if self.env.states[s] != 'Eaten' and self.env.states[s] != 'Win':\n",
    "        a = self.behavior_policy(s)\n",
    "        s_next, reward, self.done =  self.env.observations(s,a)\n",
    "\n",
    "        ## update Q function\n",
    "        self.counter[s,a] +=1\n",
    "        lr = 1/(self.counter[s,a]**self.alpha)\n",
    "        self.Q[s, a] += lr * (reward + self.discount_factor * np.max(self.Q[s_next, :]) - self.Q[s, a])\n",
    "\n",
    "        ## update next state\n",
    "        self.state = s_next\n",
    "\n",
    "    def step_SARSA(self):\n",
    "\n",
    "        ## observations\n",
    "        s = self.state\n",
    "        a = self.behavior_policy(s)\n",
    "        s_next, reward, self.done =  self.env.observations(s,a)\n",
    "        a_next = self.behavior_policy(s_next)\n",
    "\n",
    "        ## update Q function\n",
    "        self.counter[s,a] +=1\n",
    "        lr = 1/(self.counter[s,a]**self.alpha)\n",
    "        self.Q[s, a] += lr * (reward + self.discount_factor * self.Q[s_next, a_next] - self.Q[s, a])\n",
    "\n",
    "        ## update next state\n",
    "        self.state = s_next\n",
    "\n",
    "\n",
    "def dynamic_programming(env, horizon):\n",
    "    \"\"\" Solves the shortest path problem using dynamic programming\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input int horizon        : The time T up to which we solve the problem.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    ### MDP\n",
    "    P         = env.transition_probabilities\n",
    "    r         = env.rewards\n",
    "    n_states  = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    T         = horizon\n",
    "\n",
    "    ### objectives to compute\n",
    "    V = np.zeros((env.n_states, horizon+1))\n",
    "    policy = np.zeros((env.n_states, horizon+1))\n",
    "\n",
    "    #initialization\n",
    "    V[env.map['Win'], -1] = 1\n",
    "    V[env.map['Eaten'], -1] = -100\n",
    "\n",
    "    for t in range(T-1, -1, -1):\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s,a] = r[s,a]\n",
    "                for j in range(env.n_states):\n",
    "                    Q[s,a] += P[j,s,a]*V[j,t+1]\n",
    "    \n",
    "            V[s,t] = np.max(Q[s, :])\n",
    "            policy[s,t] = np.argmax(Q[s,:])\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "def value_iteration(env, gamma, epsilon):\n",
    "    \"\"\" Solves the shortest path problem using value iteration\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input float gamma        : The discount factor.\n",
    "        :input float epsilon      : accuracy of the value iteration procedure.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "\n",
    "    P = env.transition_probabilities  # Shape: (n_states, n_states, n_actions)\n",
    "    r = env.rewards                   # Shape: (n_states, n_actions)\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "\n",
    "    V = np.zeros(n_states)  # Initialize value function\n",
    "    policy = np.zeros(n_states, dtype=int)  # Initialize policy\n",
    "\n",
    "    tolerance_thr = epsilon * (1 - gamma) / gamma\n",
    "    delta = tolerance_thr + 1  # Force at least one iteration\n",
    "\n",
    "    while delta > tolerance_thr:\n",
    "        Q = np.zeros((n_states, n_actions))  # Store Q-values for all states and actions\n",
    "\n",
    "        # Compute Q-values for each action in each state\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + gamma * np.dot(P[:, s, a], V)\n",
    "\n",
    "        # Get the best value and action for each state\n",
    "        V_next = np.max(Q, axis=1)\n",
    "\n",
    "        # Update convergence criterion\n",
    "        delta = np.max(np.abs(V_next - V))\n",
    "        V = V_next\n",
    "    \n",
    "    policy = np.argmax(Q, axis=1)\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "\n",
    "def train_agent(agent, maze, start, algorithm, epsilon=0.3 ,discount_factor=49/50, alpha=2/3, num_episodes=50000):\n",
    "\n",
    "    ### initialize Q values and other parameters\n",
    "    env = Maze(maze)\n",
    "    agent = RL_agent(env, epsilon, discount_factor, alpha)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        #initialize environment and read initial state\n",
    "        env = Maze(maze)\n",
    "        s = env.map[start]\n",
    "        agent.state = s\n",
    "        agent.done = False\n",
    "\n",
    "        while not agent.done:\n",
    "            if algorithm == 'Q_learning':\n",
    "                agent.step_Q_learning()\n",
    "                print('Q_learning', i)\n",
    "            elif algorithm == 'SARSA':\n",
    "                agent.step_SARSA()\n",
    "            else:\n",
    "                raise ValueError('Unknown algorithm, choose between Q_learning and SARSA')\n",
    "    policy = agent.get_policy()\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def animate_solution(maze, path):\n",
    "\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -1: LIGHT_RED, -2: LIGHT_PURPLE, 3: LIGHT_ORANGE}\n",
    "    \n",
    "    rows, cols = maze.shape # Size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows)) # Create figure of the size of the maze\n",
    "\n",
    "    # Remove the axis ticks and add title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('Policy simulation')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(\n",
    "        cellText = None, \n",
    "        cellColours = colored_maze, \n",
    "        cellLoc = 'center', \n",
    "        loc = (0,0), \n",
    "        edges = 'closed'\n",
    "    )\n",
    "    \n",
    "    # Modify the height and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0/rows)\n",
    "        cell.set_width(1.0/cols)\n",
    "\n",
    "    for i in range(0, len(path)):\n",
    "        if path[i-1] != 'Eaten' and path[i-1] != 'Win':\n",
    "            grid.get_celld()[(path[i-1][0])].set_facecolor(col_map[maze[path[i-1][0]]])\n",
    "            grid.get_celld()[(path[i-1][1])].set_facecolor(col_map[maze[path[i-1][1]]])\n",
    "        if path[i] != 'Eaten' and path[i] != 'Win':\n",
    "            grid.get_celld()[(path[i][0])].set_facecolor(col_map[-2]) # Position of the player\n",
    "            grid.get_celld()[(path[i][1])].set_facecolor(col_map[-1]) # Position of the minotaur\n",
    "        display.display(fig)\n",
    "        time.sleep(1)\n",
    "        display.clear_output(wait = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Description of the maze as a numpy array\n",
    "    maze = np.array([\n",
    "        [0, 0, 1, 0, 0, 0, 0, 3],\n",
    "        [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 0, 0, 0, 1, 2, 0, 0]])\n",
    "    # With the convention 0 = empty cell, 1 = obstacle, 2 = exit of the Maze\n",
    "    \n",
    "    env = Maze(maze) # Create an environment maze\n",
    "    horizon = 20       # TODO: Finite horizon\n",
    "    gamma = 29/30\n",
    "    epsilon = 0.01 #10^-4\n",
    "\n",
    "    # Solve the MDP problem with dynamic programming\n",
    "    V, policy = value_iteration(env, gamma, epsilon)\n",
    "\n",
    "    # Simulate the shortest path starting from position A\n",
    "    method = 'ValIter'\n",
    "    start  = ((0,0), (6,5), False) # Start from the initial position\n",
    "    path = env.simulate(start, policy, method)[0]\n",
    "\n",
    "    animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.map[start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization: 0\n",
      "Episode: 0 Value: 0.0\n",
      "Episode: 5000 Value: 0.11182037610384256\n",
      "Episode: 10000 Value: 0.41155865492052757\n",
      "Episode: 15000 Value: 0.9584106784313172\n",
      "Episode: 20000 Value: 3.4409972909011266\n",
      "Episode: 25000 Value: 9.007956854966674\n",
      "Episode: 30000 Value: 17.00244950567401\n",
      "Episode: 35000 Value: 26.828273470984\n",
      "Episode: 40000 Value: 37.61295646495338\n",
      "Episode: 45000 Value: 48.62839786157548\n",
      "Initialization: 1\n",
      "Episode: 0 Value: 1.0\n",
      "Episode: 5000 Value: 1.0\n",
      "Episode: 10000 Value: 1.289682278472661\n",
      "Episode: 15000 Value: 2.9195752217128756\n",
      "Episode: 20000 Value: 8.096430095475972\n",
      "Episode: 25000 Value: 16.44937976139707\n",
      "Episode: 30000 Value: 26.13524764899313\n",
      "Episode: 35000 Value: 36.201392454490566\n",
      "Episode: 40000 Value: 46.785776147226294\n",
      "Episode: 45000 Value: 56.4493027447761\n",
      "Initialization: rand\n",
      "Episode: 0 Value: 0.7901736928362955\n",
      "Episode: 5000 Value: 0.9127202922797204\n",
      "Episode: 10000 Value: 0.912383789037227\n",
      "Episode: 15000 Value: 0.9133721027789659\n",
      "Episode: 20000 Value: 0.9556801532853436\n",
      "Episode: 25000 Value: 2.6831080721434297\n",
      "Episode: 30000 Value: 8.444771629864446\n",
      "Episode: 35000 Value: 18.559265383637904\n",
      "Episode: 40000 Value: 31.60050249787832\n",
      "Episode: 45000 Value: 45.791385486136704\n",
      "Initialization: 10\n",
      "Episode: 0 Value: 10.0\n"
     ]
    }
   ],
   "source": [
    "### Q learning algorithm, initialization comparison\n",
    "ini_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "epsilon=0.2\n",
    "discount_factor=49/50\n",
    "alpha=2/3\n",
    "num_episodes = 50000\n",
    "algorithm = 'Q_learning'\n",
    "inits = ['0', '1', 'rand', '10']\n",
    "v_values_init = []\n",
    "for init in inits:\n",
    "    print('Initialization:', init)\n",
    "    policy, v_values = train_agent(RL_agent, maze, start, algorithm, epsilon, discount_factor, alpha, num_episodes, init)\n",
    "    v_values_init.append(v_values)\n",
    "# Ns si s'ha d'afegir el valor del Value iteration algorithm\n",
    "\n",
    "v_values_init.append([V_opt[env.map[start]]]*num_episodes)\n",
    "inits.append('VI')\n",
    "\n",
    "plt.figure()\n",
    "for i in range(len(v_values_init)):\n",
    "    plt.plot(v_values_init[i], label='Q_init=' + inits[i])\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('V(s0)')\n",
    "# save the plot in the figures folder\n",
    "plt.savefig('figs/qi_Q_learning_init.png')\n",
    "\n",
    "print('Time:', time.time()-ini_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.2388132179521"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[env.map[start]] # Optimal value at the initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValIter\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m start  \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m5\u001b[39m), \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Start from the initial position\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m animate_solution(maze, path)\n",
      "Cell \u001b[1;32mIn[1], line 274\u001b[0m, in \u001b[0;36mMaze.simulate\u001b[1;34m(self, start, policy, method)\u001b[0m\n\u001b[0;32m    272\u001b[0m next_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__move(s, policy[s]) \u001b[38;5;66;03m# Move to next state given the policy and the current state\u001b[39;00m\n\u001b[0;32m    273\u001b[0m map_next_states \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap[state] \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m next_states] \u001b[38;5;66;03m# Map the next states\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m next_s \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_probabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmap_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose the next state given the transition probabilities\u001b[39;00m\n\u001b[0;32m    275\u001b[0m path\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[next_s]) \u001b[38;5;66;03m# Add the next state to the path\u001b[39;00m\n\u001b[0;32m    276\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Update time for next iteration\u001b[39;00m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:975\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "env = Maze(maze) # Create an environment maze\n",
    "\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'ValIter'\n",
    "start  = ((0,0), (6,5), False) # Start from the initial position\n",
    "path = env.simulate(start, policy, method)[0]\n",
    "\n",
    "animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), (6, 5), False),\n",
       " ((0, 1), (6, 6), False),\n",
       " ((1, 1), (6, 5), False),\n",
       " ((2, 1), (6, 4), False),\n",
       " ((3, 1), (5, 4), False),\n",
       " ((4, 1), (4, 4), False),\n",
       " ((4, 2), (5, 4), False),\n",
       " ((4, 3), (5, 3), False),\n",
       " ((3, 3), (6, 3), False),\n",
       " ((3, 4), (6, 2), False),\n",
       " ((2, 4), (6, 3), False),\n",
       " ((1, 4), (5, 3), False),\n",
       " ((0, 4), (5, 4), False),\n",
       " ((0, 5), (4, 4), False),\n",
       " ((0, 6), (5, 4), False),\n",
       " ((0, 7), (5, 3), True),\n",
       " ((0, 6), (4, 3), True),\n",
       " ((0, 5), (3, 3), True),\n",
       " ((0, 4), (3, 4), True),\n",
       " ((1, 4), (2, 4), True),\n",
       " ((2, 4), (2, 3), True),\n",
       " ((3, 4), (2, 4), True),\n",
       " ((4, 4), (2, 3), True),\n",
       " ((4, 5), (2, 4), True),\n",
       " ((4, 6), (3, 4), True),\n",
       " ((4, 7), (3, 3), True),\n",
       " ((5, 7), (3, 2), True),\n",
       " ((6, 7), (2, 2), True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
